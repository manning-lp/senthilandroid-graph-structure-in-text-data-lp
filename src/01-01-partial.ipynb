{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8b1a52",
   "metadata": {},
   "source": [
    "## Project 1, Milestone 1\n",
    "\n",
    "Given a corpus of text, we project the documents in the corpus to a dense, low-dimensional space and compute the cosine similarity between all pairs of documents. We then inspect the distribution of similarities and choose an appropriate similarity threshold. We then construct a graph of documents, where a pair of documens will have an edge between them if the similarity between them exceed the threshold. Finally we save the matrix as a Numpy serialized file for the next milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/project-1\"\n",
    "\n",
    "ABSTRACT_FILE = os.path.join(DATA_DIR, \"stat-abstracts.tsv\")\n",
    "\n",
    "ABS_VEC_FILE = os.path.join(DATA_DIR, \"stat-abstract-vectors.tsv\")\n",
    "DOCIDS_LIST = os.path.join(DATA_DIR, \"stat-av-docids.txt\")\n",
    "\n",
    "SIM_MATRIX_FILE = os.path.join(DATA_DIR, \"av-simmatrix.npy\")\n",
    "ADJ_MATRIX_FILE = os.path.join(DATA_DIR, \"av-adjmatrix.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231e768",
   "metadata": {},
   "source": [
    "### 1. Load the SpaCy language model\n",
    "\n",
    "We will use SpaCy's medium English language model called `en_core_web_md`. SpaCy does not load any languge models along with the library. If you see errors in the next cell complaining about invalid models, you will need to load it from the command line using the following command. See [SpaCy Models and Languages](https://spacy.io/usage/models) for more information.\n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "Once the language model is loaded successfully, you would need to restart the Jupyter notebook and relaunch this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee0bcd",
   "metadata": {},
   "source": [
    "### 2. Extract Document Vectors\n",
    "\n",
    "We are provided with a TSV file containing our corpus of papers (indicated by `ABSTRACT_TEXT`). Each line contains the `docID`, the paper `title`, a set of semi-colon separated `categories`, and the text of the `abstract`.\n",
    "\n",
    "In the `join_title_text` function below, prepend the title as the first sentence of the abstract. You can do this by adding a period to the title and joining it to the beginning of the abstract text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_title_text(title, text):\n",
    "    joined_text = None\n",
    "    ## your code goes here\n",
    "    ## Hint: join the title + \". \" + abstract_text\n",
    "\n",
    "    ## end of your code goes here\n",
    "    return joined_text\n",
    "\n",
    "\n",
    "example_joined_text = join_title_text(\"this is a title\", \"this is an abstract.\")\n",
    "example_joined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa725e",
   "metadata": {},
   "source": [
    "The `vectorize_text` function takes a block of text and a reference to the language model and returns a 300 dimensional vector. Complete the `vectorize_text` function below. If you are unsure of what function to use, refer to the [SpaCy Linguistic Features](https://spacy.io/usage/linguistic-features#vectors-similarity) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, nlp):\n",
    "    vec = None\n",
    "    ## your code goes here\n",
    "    ## Hint: passing the text to the language model returns a document object,\n",
    "    ## which has a .vector attribute that will produce the vecotr\n",
    "\n",
    "    ## end of your code goes here\n",
    "    return vec\n",
    "\n",
    "\n",
    "example_vector = vectorize_text(example_joined_text, nlp)\n",
    "example_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1db62",
   "metadata": {},
   "source": [
    "We will now read each line of the input file indicated by `ABSTRACT_FILE`, generate vectors using the (`title`, `abstract`) pairs, and write out the `docID` and generated `vector` as a string in the file indicated by `ABS_VEC_FILE`.\n",
    "\n",
    "**NOTE: This is a time consuming operation, so if you would rather skip it, you can download the `stat-abstract-vectors.tsv.backup` from the code repository for this liveProject and overwrite or replace the output of this step with it. To do this, go to your data folder and run the following commands:**\n",
    "\n",
    "```\n",
    "wget http://download.location/.../stat-abstract-vectors.tsv.backup .\n",
    "mv stat-abstract-vectors.tsv.backup stat-abstract-vectors.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dac99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 0\n",
    "fabs = open(ABSTRACT_FILE, \"r\")\n",
    "fchk = open(ABS_VEC_FILE, \"w\")\n",
    "for line in fabs:\n",
    "    if num_lines % 10000 == 0:\n",
    "        print(\"{:d} docs vectorized\".format(num_lines))\n",
    "    doc_id, title, categories, abs_text = line.strip().split('\\t')\n",
    "    vec_str = None\n",
    "    ## your code goes here\n",
    "    ## Generate the vector from the title and abstract, and stringify the vector elements\n",
    "    ## into vec_str. Each element should be represented in floating point or exponential \n",
    "    ## notation to 3 decimal places, and elements should be joined using commas (,).\n",
    "    ## Hint: consider using the {:.3e} format strint to convert each float element to string\n",
    "\n",
    "    ## end of your code goes here\n",
    "    fchk.write(\"{:s}\\t{:s}\\n\".format(doc_id, vec_str))\n",
    "    num_lines += 1\n",
    "\n",
    "print(\"{:d} docs vectorized, COMPLETE\".format(num_lines))\n",
    "fchk.close()\n",
    "fabs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151edb8",
   "metadata": {},
   "source": [
    "### 3. Construct a dense document matrix\n",
    "\n",
    "Use the file of vectors you just generated (indicated by `ABS_VEC_FILE`) and create a list of document IDs (`docids`) and a list of vectors (`vecs`). Each element of the list `vecs` must be a numpy array of size (300,).\n",
    "\n",
    "The next cell displays the size of the `docids` and `vecs` list. Verify that they are identical. Also verify that the elements of the list `vecs` are numpy vectors of size (300,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docids, vecs = [], []\n",
    "with open(ABS_VEC_FILE, \"r\") as fav:\n",
    "    num_recs = 0\n",
    "    for line in fav:\n",
    "        if num_recs % 10000 == 0:\n",
    "            print(\"{:d} vectors read\".format(num_recs))\n",
    "        doc_id, vec_str = line.strip().split('\\t')\n",
    "        ## your code goes here\n",
    "        ## Hint: split the vec_str by \",\" then cast each str element into float\n",
    "        ##       Also remember to wrap the list with np.array() to produce a numpy\n",
    "        ##       vector\n",
    "        \n",
    "        ## end of your code goes here\n",
    "\n",
    "print(\"{:d} vectors read, COMPLETE\".format(num_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of docIDs:\", len(docids))\n",
    "print(\"number of vectors:\", len(vecs))\n",
    "print(\"shape of vector in vecs:\", vecs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af32ad",
   "metadata": {},
   "source": [
    "We now convert the list of (300,) vectors into a matrix of documents. There are 50426 documents in the corpus, so verify that the shape of the matrix is (50426, 300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vecs)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976ae1e",
   "metadata": {},
   "source": [
    "### 4. Construct a document similarity matrix\n",
    "\n",
    "We use cosine similarity as our similarity measure. The formula for cosine similarity between a pair of documents vectors $d_1$ and $d_2$ is given by:\n",
    "\n",
    "$$cosim(d_1, d_2) = \\frac{d_1 \\cdot d_2}{{\\lvert d_1 \\rvert}_2 {\\lvert d_2 \\rvert}_2}$$\n",
    "\n",
    "We can also compute the cosine similarity between all pairs of documents in the document matrix X using the following formula.\n",
    "\n",
    "$$S = \\frac{X \\cdot X^T}{{\\lvert X \\rvert}_2^2}$$\n",
    "\n",
    "Since the denominator on the RHS is a constant, we can re-state the equation above as:\n",
    "\n",
    "$$S \\propto X \\cdot X^T$$\n",
    "\n",
    "\n",
    "\n",
    "**NOTE: this is a time consuming operation. If you would rather skip this, please download the serialized version of the similarity matrix from the code repository for this liveProject and load it into the variable `S` by uncommenting the commented code in the next cell instead (the one containing S = np.load(SIM_MATRIX_FILE).**\n",
    "\n",
    "```\n",
    "wget http://download.location/.../av-simmatrix.npy .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.dot(X, X.T)\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599190dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = np.load(SIM_MATRIX_FILE)\n",
    "# S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5c269",
   "metadata": {},
   "source": [
    "### 5. Determine Similarity Threshold\n",
    "\n",
    "We sample around 1000 elements from the similarity matrix and plot a histogram to get an idea of the distribution of cosine similarity scores. \n",
    "\n",
    "We want to draw edges only between documents with relatively high similarity. Based on the histogram, a good threshold seems to be 9.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = np.random.randint(0, S.shape[0], 1000)\n",
    "col_indices = np.random.randint(0, S.shape[1], 1000)\n",
    "samples = []\n",
    "for row, col in zip(row_indices, col_indices):\n",
    "    samples.append(S[row, col])\n",
    "\n",
    "plt.hist(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8629a",
   "metadata": {},
   "source": [
    "### 6. Create Adjacency Matrix\n",
    "\n",
    "We can now create an adjacency matrix `A` from the similarity matrix `S`. An adjacency matrix is a square matrix of the same size as the similarity matrix, i.e. an (N, N) matrix where N is the number of documents. An element `A[i, j]` is 1 if there is high similarity between $doc_i$ and $doc_j$, i.e. similarity above the threshold.\n",
    "\n",
    "Also remember to set the diagonal elements of the adjacency matrix to 0. For similarity matrices, the highest values are on the diagonal, since a document is most similar to itself. However, that would translate to self-loops in a graph, which we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8967f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 9.5\n",
    "\n",
    "A = np.zeros(S.shape)\n",
    "ones_indices = S >= threshold\n",
    "A[ones_indices] = 1\n",
    "\n",
    "np.fill_diagonal(A, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c47a26a",
   "metadata": {},
   "source": [
    "### 7. Save Adjacency Matrix and DocIDs list\n",
    "\n",
    "Our adjacency matrix `A` is now sparse, so you can either save it directly using `np.save()` or convert it to a SciPy sparse COO Matrix using `coo_matrix(A.astype(np.int8)` and then save it using `save_npz()`. Saving it as a sparse matrix is recommended, since it will take about the same time to save to disk, but will result in a much smaller disk image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(ADJ_MATRIX_FILE, A)\n",
    "save_npz(ADJ_MATRIX_FILE, coo_matrix(A.astype(np.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2675f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdocs = open(DOCIDS_LIST, \"w\")\n",
    "for i, doc_id in enumerate(docids):\n",
    "    fdocs.write(\"{:s}\\t{:d}\\n\".format(doc_id, i))\n",
    "fdocs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4549708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
