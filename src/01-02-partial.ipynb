{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79120c6e",
   "metadata": {},
   "source": [
    "# Project 1, Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7134baf",
   "metadata": {},
   "source": [
    "## 1. Install Neo4j server\n",
    "\n",
    "Consult the Workflow section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211e247",
   "metadata": {},
   "source": [
    "## 2. Configure Neo4j server\n",
    "\n",
    "Consult the Workflow section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3595a",
   "metadata": {},
   "source": [
    "## 3. Create new database\n",
    "\n",
    "Consult the Workflow section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33665ef0",
   "metadata": {},
   "source": [
    "## 4. Create the graph\n",
    "\n",
    "In this step, we will convert the adjacency matrix we created in the previous milestone and combine it with our original dataset to create a pair of files of the form shown below. These will serve as input files to the `neo4j-admin import` command to load this data into a Neo4j graph.\n",
    "\n",
    "**nodes.csv**\n",
    "\n",
    "```\n",
    "doc_id:ID,title,category,:LABEL\n",
    "0704.0302,Spline Single-Index Prediction Model,stat.TH,Article\n",
    "0704.0326,On generalized entropy measures and pathways,stat.TH,Article\n",
    "...\n",
    "```\n",
    "\n",
    "**edges.csv**\n",
    "```\n",
    ":START_ID,:END_ID,:TYPE\n",
    "0704.0302,1802.02649,SIMILAR_TO\n",
    "0704.0517,0704.0744,SIMILAR_TO\n",
    "...\n",
    "```\n",
    "\n",
    "Note that the first line in each file describes metadata that will be used to populate node and edge properties.\n",
    "\n",
    "For example, the first line in **nodes.csv** tells us that each node will have a `docId` attribute that uniquely identifies it, hence can be used as an `:ID`, a `title` and `category` attributes, both of which are of type `str`. The type of node is identified by the `:LABEL` attribute, here we say that our nodes would be called `Article`. The subsequent lines describe the corresponding values for each node in our graph.\n",
    "\n",
    "Similarly, the first line of **edges.csv** tells us that the subsequent lines are composed of a pair of nodes indicated by the values of their `docId:ID` property, the `:START_ID` and `:END_ID` respectively, and the type of edge connecting these two `Article` nodes is called `SIMILAR_TO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbed7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.sparse import load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34672865",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/project-1\"\n",
    "\n",
    "PATTERN = \"av\"\n",
    "\n",
    "# our inputs from previous milestones\n",
    "# input to milestone 1\n",
    "ABSTRACTS_FILE = os.path.join(DATA_DIR, \"stat-abstracts.tsv\")\n",
    "# outputs of milestone 1\n",
    "DOCIDS_FILE = os.path.join(DATA_DIR, \"stat-{:s}-docids.txt\".format(PATTERN))\n",
    "INPUT_MATRIX_FILE = os.path.join(DATA_DIR, \"{:s}-adjmatrix.npz\".format(PATTERN))\n",
    "\n",
    "# our deliverables from milestone 2\n",
    "NODE_FILE = os.path.join(DATA_DIR, \"{:s}-node.csv\".format(PATTERN))\n",
    "RELS_FILE = os.path.join(DATA_DIR, \"{:s}-edges.csv\".format(PATTERN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97811a42",
   "metadata": {},
   "source": [
    "### Deserialize mapping of row/col id to docID\n",
    "\n",
    "Our adjacency matrix we generated in the previous milestone is available to us as a numpy serialized file. On deserializing, we will get a square matrix of size (50426, 50426), where 50426 is the number of articles provided to us in our initial dataset.\n",
    "\n",
    "Since we need to think in terms of a graph of actual documents now, we would like to relate the row / column ids in the adjacency matrix to actual articles and their document IDs. Therefore we need to produce a mapping of row / column ids to their corresponding article IDs.\n",
    "\n",
    "Read the file indicated by `DOCIDS_FILE` and extract from it a dictionary mappng the row / column ID to the article `docID`. Call this dictionary `id_to_docid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945854ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_docid = {}\n",
    "\n",
    "## your code goes here\n",
    "## Hint: Open the DOCIDS_FILE, loop through it, extracting docID and id values\n",
    "## and populate a dictionary\n",
    "\n",
    "## end of your code goes here\n",
    "len(id_to_docid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da65daa",
   "metadata": {},
   "source": [
    "### Create mapping of docID to (title, category)\n",
    "\n",
    "In order to create our **nodes.csv** file, we need the ability to look up the document title and category from the input file (indicated by `ABSTRACTS_FILE`) provided to us.\n",
    "\n",
    "The `ABSTRACTS_FILE` contains the `docID`, `title`, `categories` and `abstract_text` for each article. To keep things simple, we will set the article category to the first `stat` category that the authors have marked it up with. Write a function that will take a concatenated set of categories separated by the semi-colon (\";\") character, and select the first category that starts with the string \"stat.\". Assign this category to the variable `first_stat_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_stat_category(categories_str):\n",
    "    first_stat_cat = None\n",
    "    ### your code goes here\n",
    "\n",
    "    ### end of your code goes here\n",
    "    return first_stat_cat\n",
    "\n",
    "\n",
    "# test your function, you should get stat.SE\n",
    "get_first_stat_category(\"cs.ML;stat.SE;stat.TH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d800e",
   "metadata": {},
   "source": [
    "Next, we will parse the `ABSTRACTS_FILE` to extract `docID`, `title` and `categories` fields. We will extract the first `stat.*` category from the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_to_title_cat = {}\n",
    "\n",
    "num_docs = 0\n",
    "with open(ABSTRACTS_FILE, \"r\") as fabs:\n",
    "    for line in fabs:\n",
    "        if num_docs % 10000 == 0:\n",
    "            print(\"{:d} doc IDs read\".format(num_docs))\n",
    "        ## your code here\n",
    "        ## Hint: Get docId, title and categories, then extract the first stat. category\n",
    "\n",
    "        ## end of your code here\n",
    "        docid_to_title_cat[doc_id] = (title, stat_cat)\n",
    "        num_docs += 1\n",
    "        \n",
    "print(\"{:d} doc IDs read, COMPLETE\".format(num_docs))\n",
    "len(docid_to_title_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c721c",
   "metadata": {},
   "source": [
    "### Create the nodes CSV file\n",
    "\n",
    "Using the `id_to_docid` and `docid_to_title_cat` dictionaries you just created, write out the nodes CSV file **nodes.csv** indicated by `NODE_FILE` above.\n",
    "\n",
    "As a reminder, here is what the output should look like.\n",
    "\n",
    "**nodes.csv**\n",
    "\n",
    "```\n",
    "doc_id:ID,title,category,:LABEL\n",
    "0704.0302,Spline Single-Index Prediction Model,stat.TH,Article\n",
    "0704.0326,On generalized entropy measures and pathways,stat.TH,Article\n",
    "...\n",
    "```\n",
    "\n",
    "Make sure to handle commas within the `title` field, either by quoting the title or replacing the comma character in the title with some other character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84149409",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 0\n",
    "fnode = open(NODE_FILE, \"w\")\n",
    "## your code goes here\n",
    "## write out the header as shown in the example\n",
    "\n",
    "## end of your code goes here\n",
    "for i in range(len(id_to_docid)):\n",
    "    if num_docs % 10000 == 0:\n",
    "        print(\"{:d} nodes written\".format(num_docs))\n",
    "    ## your code goes here\n",
    "    ## write out each line as shown in the example\n",
    "\n",
    "    ## end of your code goes here\n",
    "    num_docs += 1\n",
    "    \n",
    "print(\"{:d} nodes written, COMPLETE\\n\".format(num_docs))\n",
    "fnode.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fdb6e",
   "metadata": {},
   "source": [
    "### Check your work\n",
    "\n",
    "The `check_csv` function below checks that your CSV file contains the correct number of columns and rows, otherwise it will error out and give you the line number in the file where there was a problem. We expect our `node.csv` file to have 4 columns per line and 50426 lines, since we are extracting the attributes `docID`, `title`, `category` and `type` for each of 50426 articles in our dataset.\n",
    "\n",
    "**If your node.csv file looks good, you should see NO OUTPUT from the `check_csv` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1920b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csv(file_path, expected_num_cols, expected_num_rows):\n",
    "    with open(file_path, \"r\") as fcsv:\n",
    "        reader = csv.reader(fcsv)\n",
    "        for i, row in enumerate(reader):\n",
    "            if len(row) != expected_num_cols:\n",
    "                print(\"Invalid CSV file: {:d} columns in line {:d}, expected {:d}\"\n",
    "                      .format(len(row), i+1, expected_num_cols))\n",
    "                break\n",
    "        if i != expected_num_rows:\n",
    "            print(\"Invalid CSV file: {:d} rows found, expected {:d}\".format(i, expected_num_rows))\n",
    "            \n",
    "\n",
    "check_csv(NODE_FILE, 4, 50426)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1d220",
   "metadata": {},
   "source": [
    "### Deserialize the Adjacency Matrix\n",
    "\n",
    "Deserialize the Adjacency Matrix you created from the last milestone (indicated by `INPUT_MATRIX_FILE`). Verify that the shape of the matrix is `(50426, 50426)`.\n",
    "\n",
    "If you have saved the adjacency matrix as a dense matrix (i.e., using `np.save()`) in the last milestone, then you should use `np.load()` to deserialize. If you saved it as a sparse COO matrix (our recommended approach), then you should use `load_npz()` instead. Deserializing from a sparse representation is significantly faster than deserializing from a dense representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here\n",
    "\n",
    "## end of your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1ddac",
   "metadata": {},
   "source": [
    "### Create the edges.csv file\n",
    "\n",
    "Use the deserialized matrix `A` and our `id_to_docid` dictionary to create the **edges.csv** file indicated by `RELS_FILE`. As a reminder, here is what the output should look like.\n",
    "\n",
    "```\n",
    ":START_ID,:END_ID,:TYPE\n",
    "0704.0302,1802.02649,SIMILAR_TO\n",
    "0704.0517,0704.0744,SIMILAR_TO\n",
    "...\n",
    "```\n",
    "\n",
    "Remember that graph edges in this situation is based on document similarity, and is hence symmetric, i.e. if articles $doc_i$ and $doc_j$ are similar according to the adjacency matrix, then `A[i, j] == A[j, i]`. While Neo4j supports directed edges only, the Cypher query language can ignore the edge direction when querying the graph. Therefore, when creating the graph, we will specify only a single edge between any given $doc_i$, $doc_j$ pair.\n",
    "\n",
    "Also remember that typically adjacency matrices that are designed based on similarity have the highest values along the diagonal (i.e. a document is most similar with itself), so in the general case you should correct for that as well. \n",
    "\n",
    "A good way to do this is to look only at non-zero entries in `A` where `i < j`, i.e., \n",
    "\n",
    "```\n",
    "if i < j and A[i, j] > 0:\n",
    "    # add an edge between i and j\n",
    "```\n",
    "\n",
    "**NOTE: This operation will take some time to complete!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs, num_edges = 0, 0\n",
    "frels = open(RELS_FILE, \"w\")\n",
    "frels.write(\":START_ID,:END_ID,:TYPE\\n\")\n",
    "for i in range(A.shape[0]):\n",
    "    if i % 10000 == 0:\n",
    "        print(\"{:d} docs read\".format(num_docs))\n",
    "    ## your code goes here\n",
    "    ## Find the start and end docIDs from the indexes i and j\n",
    "    ## Remember that our graph is based on document similarity and hence symmetric\n",
    "    ## Also remember to skip diagonal elements (see if condition above)\n",
    "\n",
    "    ## end of your code goes here\n",
    "            num_edges += 1\n",
    "    num_docs += 1\n",
    "\n",
    "print(\"{:d} docs read, COMPLETE\".format(num_docs))\n",
    "print(\"--\")\n",
    "print(\"{:d} edges written, COMPLETE\".format(num_edges))\n",
    "frels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5b263",
   "metadata": {},
   "source": [
    "### Check your work\n",
    "\n",
    "**If your edges.csv file looks good, you should see no output from the call to `check_csv` below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ec0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_csv(RELS_FILE, 3, 25603984)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e83ab",
   "metadata": {},
   "source": [
    "### Compute graph sparsity\n",
    "\n",
    "As a fun exercise, compute the graph sparsity. That is compute the ratio of `num_edges` and the maximum number of possible edges for a fully connected graph.\n",
    "\n",
    "The maximum number of edges possible would be `num_nodes` * `(num_nodes - 1)`. This will give us two edges between any two nodes i and j. However, since the `SIMILAR_TO` relationship is symmetric, we should consider only one edge.\n",
    "\n",
    "Your answer rounded to 2 decimal places should be `0.02`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5901fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "## end of your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7877fa",
   "metadata": {},
   "source": [
    "### Load data into neo4j\n",
    "\n",
    "While there are multiple ways to load data into a Neo4j database, by far the fastest is the `neo4j-admin import` command.\n",
    "\n",
    "**NOTE: This requires the Neo4j server to be shut down.**\n",
    "\n",
    "First shut down the neo4j server, then run the command to import the data into Neo4j, then restart the server.\n",
    "\n",
    "```\n",
    "$NEO4J_HOME/bin/neo4j stop\n",
    "$NEO4J_HOME/bin/neo4j-admin import --database=av-graph --nodes=av-nodes.csv --relationships=av-edges.csv\n",
    "```\n",
    "\n",
    "We are using Neo4j community edition, which allows only a single graph database accessible at a time, so we need to make the default database `av-graph` in the configuration. In $NEO4J_HOME/conf/neo4j.conf, set the value of `dbms.default_database` to `av-graph`.\n",
    "\n",
    "```\n",
    "# $NEO4J_HOME/conf/neo4j.conf\n",
    "...\n",
    "dbms.default_database=av-graph\n",
    "...\n",
    "```\n",
    "\n",
    "Then restart the Neo4j server.\n",
    "\n",
    "```\n",
    "$NEO4J_HOME/bin/neo4j start|console\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ce939",
   "metadata": {},
   "source": [
    "## 5. Run Basic Exploratory Commands\n",
    "\n",
    "You should be able to verify that the data was successfully loaded. You can use the Neo4j browser interface at http://localhost:7474 to count the number of nodes and edges using the following Cypher commands.\n",
    "\n",
    "```\n",
    "MATCH (n) RETURN COUNT(n) as num_nodes\n",
    "```\n",
    "\n",
    "```\n",
    "MATCH ()-[r]->() RETURN COUNT(r) as num_edges\n",
    "```\n",
    "\n",
    "Alternatively, you can also try using the `py2neo` interface to do this programatically from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"admin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_g = graph.run(\"MATCH (n) RETURN COUNT(n) AS num_nodes\")\n",
    "num_nodes_g = list(num_nodes_g.data())\n",
    "num_nodes_g[0][\"num_nodes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede57a45",
   "metadata": {},
   "source": [
    "Use py2neo to find the number of edges in the graph, using the Cypher query shown earlier to find the number of edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38924bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "num_edges_g = graph.run(\"MATCH ()-[r]->() RETURN COUNT(r) AS num_edges\")\n",
    "num_edges_g = list(num_edges_g.data())\n",
    "num_edges_g[0][\"num_edges\"]\n",
    "## end of your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747ce3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
